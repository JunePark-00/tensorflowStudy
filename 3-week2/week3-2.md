# Word Embedding

> **2주차 정리**
>
> - 토큰화된 단어를 가져오고 임베딩을 사용하여 수학적 방식으로 의미를 설정하는 방법
>
> - 단어는 고차원 공간의 벡터에 매핑되고 단어의 의미는 해당 단어에 유사한 의미로 레이블이 지정되었을 때 학습되었음
>
> - 예시) 영화 리뷰를 볼 때 긍정적인 감정을 가진 영화는 단어의 차원이 특정 방향을 가리키는 것으로 끝나고, 부정적인 감정이 있는 영화는 다른 방향을 가리키는 차원
>
> - -> 미래 문장의 단어들은 '방향'을 정할 수 있고, 이를 통해 감정을 유추할 수 있음. 
>
> - 그런 다음 하위 단어 토큰화를 살펴보고 단어의 의미뿐만 아니라 단어가 발견되는 순서도 중요하다는 것을 배움.
>
>   
>
> **Word Embedding**
>
> 단어 임베딩은 유사한 단어가 유사한 인코딩을 갖는 효율적이고 조밀한 표현을 사용하는 방법을 제공한다. 중요한 것은, **이 인코딩을 직접 지정할 필요가 없다**는 것이다. 임베딩은 부동 소수점 값으로 구성된 조밀한 벡터이다(벡터의 길이는 사용자가 지정하는 매개 변수이다). 임베딩 값을 수동으로 지정하는 대신, 학습 가능한 매개 변수이다(모델이 조밀한 레이어에 대한 가중치를 학습하는 것과 같은 방식으로 학습 중에 모델에서 학습한 가중치). 작은 데이터 세트의 경우 8차원, 대규모 데이터 세트로 작업할 때 최대 1024차원의 단어 임베딩을 보는 것이 일반적이다. 더 높은 차원의 임베딩은 단어 간의 세밀한 관계를 포착할 수 있지만, 학습하는 데 더 많은 데이터가 필요하다.
>
> ![임베딩 다이어그램](https://www.tensorflow.org/text/guide/images/embedding2.png)
>
> 위는 단어 임베딩에 관한 다이어그램이다. 각 단어는 부동 소수점 값의 4차원 벡터로 표시된다. 임베딩을 생각하는 또 다른 방법은 조회 테이블이다. 이러한 가중치를 학습한 후 표에서 해당하는 조밀한 벡터를 찾아 각 단어를 인코딩할 수 있다.
>
> * Embedding Layer 사용
>
> ~~~python
> model = tf.keras.Sequential([
>     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
>     tf.keras.layers.Flatten(),
>     tf.keras.layers.Dense(6, activation='relu'),
>     tf.keras.layers.Dense(1, activation='sigmoid')
> ])
> model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
> model.summary()
> ~~~
>
> 케라스를 사용하면 단어 임베딩을 쉽게 사용할 수 있다. 
>
> 임베딩 레이어는 정수 인덱스(특정 단어를 의미)에서 고밀도 벡터(임베딩)로 매핑하는 조회 테이블로 이해할 수 있다. 임베딩의 차원(또는 너비)은 Dense 레이어의 뉴런 수를 실험하는 것과 같은 방식으로 문제에 잘 맞는 지 확인하기 위해 실험할 수 있는 매개 변수이다.
>
> 임베딩 레이어를 만들면 다른 레이어와 마찬가지로 임베딩에 대한 가중치가 임의로 초기화된다. 훈련 중에 backpropagation을 통해 점진적으로 조정된다. 학습된 단어 임베딩은 모델이 학습된 특정 문제에 대해 학습된 것처럼 단어 간의 유사성을 대략적으로 인코딩한다.