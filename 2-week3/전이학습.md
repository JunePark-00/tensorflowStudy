# Transfer Learning

Transfer Learning(전이학습)

: 이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치의 일부를 능력이 유사하거나 새로운 분야의 신경망에 복사한 후, 그 상태로 재학습을 수행하는 것. 학습 데이터가 부족한 프로젝트를 진행하는 경우에 큰 도움이된다.

Example)

동양인과 서양인을 구분하는 신경망 구현 시, 남녀를 구분하는 신경망에서 이미지의 특징을 추출하는 부분에 대한 파라미터들을 가져와 나머지를 내 데이터로 학습시켜 완성도를 높인다.



> 사전 훈련된 모델을 사용자 정의하는 두 가지 방법
>
> 1. 특징 추출 : 새 샘플에서 의미 있는 형상을 추출하기 위해 이전 네트워크에서 학습한 표현을 사용합니다. 사전 훈련된 모델 위에 처음부터 교육할 새 분류기를 추가하기만 하면 이전에 데이터셋으로 학습한 특징 맵의 용도를 재사용할 수 있습니다. 전체 모델을 재훈련시킬 필요는 없습니다. 기본 컨볼루션 네트워크에는 그림 분류에 일반적으로 유용한 기능이 이미 포함되어 있습니다. 그러나 사전 훈련된 모델의 최종 분류 부분은 기존의 분류 작업에 따라 다르며 이후에 모델이 훈련된 클래스 집합에 따라 다릅니다.
> 2. 미세 조정: 고정된 기본 모델의 일부 최상위 층을 고정 해제하고 새로 추가 된 분류기 층과 기본 모델의 마지막 층을 함께 훈련시킵니다. 이를 통해 기본 모델에서 고차원 특징 표현을 "미세 조정"하여 특정 작업에 보다 관련성이 있도록 할 수 있습니다.



> 일반적인 기계학습 일련의 과정을 진행
>
> 1. 데이터 검사 및 이해
> 2. 입력 파이프 라인 빌드(이 경우 Keras ImageDataGenerator를 사용)
> 3. 모델 작성
>    - 사전 훈련된 기본 모델(또한 사전 훈련된 가중치)에 적재
>    - 분류 층을 맨 위에 쌓기
> 4. 모델 훈련
> 5. 모델 평가



> 데이터 전처리
>
> ```python
> import tensorflow_datasets as tfds
> tfds.disable_progress_bar()
> ```
>
> [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load?hl=ko) 메소드는 데이터를 다운로드 및 캐시하고 [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ko) 오브젝트를 리턴합니다. 이러한 객체는 데이터를 조작하고 모델에 파이프하는 강력하고 효율적인 방법을 제공합니다.
>
> `"cats_vs_dogs"` 는 표준 splits 기능을 정의하지 않으므로 subsplit 기능을 사용하여 각각 80%, 10%, 10%(훈련, 검증, 테스트)의 데이터로 나눕니다.
>
> ```python
> (raw_train, raw_validation, raw_test), metadata = tfds.load(
>     'cats_vs_dogs',
>     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
>     with_info=True,
>     as_supervised=True,
> )
> ```
>
> tf.data.Dataset 객체에는 (이미지, 레이블) 쌍으로 이루어져 있습니다. 이미지는 가변적인 shape, 3개 채널로 구성되며, 레이블은 스칼라로 구성됩니다.
>
> 훈련용 데이터셋에서 처음 두 개의 이미지 및 레이블을 보여줍니다:
>
> ```python
> get_label_name = metadata.features['label'].int2str
> 
> for image, label in raw_train.take(2):
>   plt.figure()
>   plt.imshow(image)
>   plt.title(get_label_name(label))
> ```
>
> ![png](https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_K5BeQyKThC_Y_0.png?hl=ko)
>
> ![png](https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_K5BeQyKThC_Y_1.png?hl=ko)



> 데이터 포맷
>
> [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image?hl=ko) 모듈을 사용하여 이미지를 포맷.
>
> 이미지를 고정 된 입력 크기로 조정하고 입력 채널의 크기를 `[-1,1]` 범위로 조정.
>
> ```python
> IMG_SIZE = 160 # 모든 이미지는 160x160으로 크기가 조정됩니다
> 
> def format_example(image, label):
>   image = tf.cast(image, tf.float32)
>   image = (image/127.5) - 1
>   image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
>   return image, label
> ```
>
> map 함수를 사용하여 데이터셋의 각 항목에 이 함수를 적용:
>
> ```python
> train = raw_train.map(format_example)
> validation = raw_validation.map(format_example)
> test = raw_test.map(format_example)
> ```
>
> 데이터 섞고 일괄 처리
>
> ```python
> BATCH_SIZE = 32
> SHUFFLE_BUFFER_SIZE = 1000
> ```
>
> ```python
> train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
> validation_batches = validation.batch(BATCH_SIZE)
> test_batches = test.batch(BATCH_SIZE)
> ```
>
> 일련의 데이터 검사:
>
> ```python
> for image_batch, label_batch in train_batches.take(1):
>    pass
> 
> image_batch.shape
> ```



> 사전 훈련된 Convolution Network로부터 기본 모델 생성
>
> ```python
> IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)
> 
> # 사전 훈련된 모델 MobileNet V2에서 기본 모델을 생성합니다.
> base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
>                                                include_top=False,
>                                                weights='imagenet')
> ```
>
> ```python
> feature_batch = base_model(image_batch)
> print(feature_batch.shape)
> ```



> 특징 추출
>
> 이 단계에서는 이전 단계에서 작성된 컨벌루션 베이스 모델을 동결하고 특징 추출기로 사용합니다. 또한 그 위에 분류기를 추가하고 최상위 분류기를 훈련시킵니다.
>
> 
>
> 컨볼루션 베이스 모델 고정:
>
> 모델을 컴파일하고 훈련시키기 전에 컨볼루션 베이스 모델을 고정 시키는 것이 중요합니다. 고정(layer.trainable = False를 설정하여)하면 훈련 중 지정된 층의 가중치가 업데이트되지 않습니다. MobileNet V2에는 많은 층이 있으므로 전체 모델의 훈련 가능한 플래그를 False로 설정하면 모든 층이 고정됩니다.
>
> ```python
> base_model.trainable = False
> ```
>
> 분류 층을 맨 위에 추가:
>
> 특징 블록에서 예측을 하기위해 tf.keras.layers.GlobalAveragePooling2D 층을 사용하여 특징을 이미지 한개 당 1280개의 요소 벡터로 변환하여 5x5 공간 위치에 대한 평균을 구합니다.
>
> ```python
> global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
> feature_batch_average = global_average_layer(feature_batch)
> print(feature_batch_average.shape)
> ```
>
> [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=ko)층을 사용하여 특징을 이미지당 단일 예측으로 변환합니다. 이 예측은 `logit`또는 원시 예측 값으로 취급되므로 활성화 함수가 필요하지 않습니다. 양수는 클래스 1을 예측하고 음수는 클래스 0을 예측합니다.
>
> ```python
> prediction_layer = keras.layers.Dense(1)
> prediction_batch = prediction_layer(feature_batch_average)
> print(prediction_batch.shape)
> ```



> 모델 컴파일
>
> 학습하기 전에 모델을 컴파일해야 합니다. 두 개의 클래스가 있으므로 모델이 선형 출력을 제공하므로 `from_logits = True`와 함께 이진 교차 엔트로피 손실을 사용합니다.
>
> ```python
> base_learning_rate = 0.0001
> model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),
>               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
>               metrics=['accuracy'])
> ```



> 모델 트레이닝
>
> 아래 코드는 10 epochs만큼 훈련 후 ~96%의 정확도를 볼 수 있습니다.
>
> ```python
> initial_epochs = 10
> validation_steps=20
> 
> loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)
> ```
>
> ```python
> history = model.fit(train_batches,
>                     epochs=initial_epochs,
>                     validation_data=validation_batches)
> ```



> 학습 곡선
>
> ```python
> acc = history.history['accuracy']
> val_acc = history.history['val_accuracy']
> 
> loss = history.history['loss']
> val_loss = history.history['val_loss']
> 
> plt.figure(figsize=(8, 8))
> plt.subplot(2, 1, 1)
> plt.plot(acc, label='Training Accuracy')
> plt.plot(val_acc, label='Validation Accuracy')
> plt.legend(loc='lower right')
> plt.ylabel('Accuracy')
> plt.ylim([min(plt.ylim()),1])
> plt.title('Training and Validation Accuracy')
> 
> plt.subplot(2, 1, 2)
> plt.plot(loss, label='Training Loss')
> plt.plot(val_loss, label='Validation Loss')
> plt.legend(loc='upper right')
> plt.ylabel('Cross Entropy')
> plt.ylim([0,1.0])
> plt.title('Training and Validation Loss')
> plt.xlabel('epoch')
> plt.show()
> ```
>
> 위의 코드를 실행시키면 아래와 같은 결과를 얻을 수 있습니다.
>
> ![png](https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_53OTCh3jnbwV_0.png?hl=ko)
>
> 유효성 검사 지표가 훈련 지표보다 명확하게 더 나은 이유는 [`tf.keras.layers.BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?hl=ko) 및 [`tf.keras.layers.Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout?hl=ko)과 같은 층이 훈련 중 정확도에 영향을 주기 때문입니다. 이것들은 유효성 검사 손실을 계산할 때 해제됩니다.
>
> 훈련 지표가 한 에포크 동안의 평균을 평가하는 반면, 유효성 검사 지표는 에포크 이후에 평가하므로 유효성 검사 지표는 약간 더 많이 훈련된 모델을 볼 수 있습니다.



> 미세 조정
>
> 기능 추출 실험에서는 MobileNet V2 기본 모델을 기반으로 몇 개의 층 만 학습했습니다. 사전 훈련된 네트워크의 가중치는 훈련 중에 업데이트 되지 **않았습니다**.
>
> 성능을 더욱 향상시키는 한 가지 방법은 추가 한 분류기의 훈련과 함께 사전 훈련된 모델의 최상위 레이어 가중치를 훈련(또는 "미세 조정")하는 것입니다. 훈련을 통해 가중치는 일반적인 특징 맵에서 개별 데이터셋과 관련된 특징으로 조정됩니다.
>
> 사전 훈련된 모델을 훈련 불가능으로 설정하여 최상위 분류기를 훈련한 후에만 시도해야 합니다. 사전 훈련된 모델 위에 무작위로 초기화된 분류기를 추가하고 모든 레이어를 공동으로 훈련하려고하면 (분류기가 가중치를 임의 설정하기 때문에) 그래디언트 업데이트의 크기가 너무 커지고 사전 훈련된 모델은 배운 것을 잊어버리게 됩니다.
>
> 또한 전체 MobileNet 모델이 아닌 소수의 최상위 층을 미세 조정해야 합니다. 대부분의 컨볼루션 네트워크에서 층이 높을수록 층이 더 전문화됩니다. 처음 몇 층은 거의 모든 유형의 이미지로 일반화되는 매우 간단하고 일반적인 특징을 학습합니다. 더 높은 수준으로 올라가면 훈련에 사용된 데이터 세트에 맞춰 특징이 점점 더 구체화 됩니다. 미세 조정의 목표는 이러한 전문화된 특징이 일반적인 학습을 덮어쓰지 않고 새 데이터셋에 맞춰 잘 동작 수 있도록 조정하는 것입니다.
>
> 최상위 층 고정 해제:
>
> base_model을 고정 해제하고 맨 아래 층을 훈련 할 수 없도록 설정하면 됩니다. 그런 다음 모델을 다시 컴파일하고(변경 사항을 적용하기 위해서) 훈련을 다시 시작해야 합니다.
>
> ```python
> base_model.trainable = True
> ```
>
> ```python
> # 기본 모델에 몇 개의 층이 있는지 확인 합니다.
> print("Number of layers in the base model: ", len(base_model.layers))
> 
> # 해당 층 이후부터 미세 조정
> fine_tune_at = 100
> 
> # `fine_tune_at` 층 이전의 모든 층을 고정
> for layer in base_model.layers[:fine_tune_at]:
>   layer.trainable =  False
> ```



> 모델 컴파일
>
> 훨씬 더 낮은 학습 비율로 모델 컴파일 합니다.
>
> ```python
> model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
>               optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),
>               metrics=['accuracy'])
> ```



> 모델 훈련 계속하기
>
> 이미 수렴 상태로 훈련된 경우에, 이 단계는 정확도를 몇 퍼센트 포인트 향상시킵니다.
>
> ```python
> fine_tune_epochs = 10
> total_epochs =  initial_epochs + fine_tune_epochs
> 
> history_fine = model.fit(train_batches,
>                          epochs=total_epochs,
>                          initial_epoch =  history.epoch[-1],
>                          validation_data=validation_batches)
> ```
>
> 새로운 훈련용 데이터셋이 상대적으로 작고 원래 MobileNet V2의 데이터셋과 유사하기 때문에 약간의 과적합이 발생할 수 있습니다.
>
> 미세 조정 후 모델은 거의 98% 정확도에 도달합니다.
>
> ```python
> acc += history_fine.history['accuracy']
> val_acc += history_fine.history['val_accuracy']
> 
> loss += history_fine.history['loss']
> val_loss += history_fine.history['val_loss']
> ```
>
> ```python
> plt.figure(figsize=(8, 8))
> plt.subplot(2, 1, 1)
> plt.plot(acc, label='Training Accuracy')
> plt.plot(val_acc, label='Validation Accuracy')
> plt.ylim([0.8, 1])
> plt.plot([initial_epochs-1,initial_epochs-1],
>           plt.ylim(), label='Start Fine Tuning')
> plt.legend(loc='lower right')
> plt.title('Training and Validation Accuracy')
> 
> plt.subplot(2, 1, 2)
> plt.plot(loss, label='Training Loss')
> plt.plot(val_loss, label='Validation Loss')
> plt.ylim([0, 1.0])
> plt.plot([initial_epochs-1,initial_epochs-1],
>          plt.ylim(), label='Start Fine Tuning')
> plt.legend(loc='upper right')
> plt.title('Training and Validation Loss')
> plt.xlabel('epoch')
> plt.show()
> ```
>
> ![png](https://www.tensorflow.org/tutorials/images/transfer_learning_files/output_chW103JUItdk_0.png?hl=ko)



## Summary ##

- **특징 추출을 위해 사전 훈련된 모델 사용하기**: 작은 데이터셋으로 작업 할 때는 동일한 범주의 클래스의 더 큰 데이터셋으로 훈련시킨 사전 학습된 모델의 특징을 활용하는 것이 일반적입니다. 사전 훈련된 모델을 인스턴스화하고 완전히 연결된 분류기를 맨 위에 추가하면 됩니다. 사전 훈련된 모델은 "고정"되고 분류기의 가중치만 훈련 중에 업데이트됩니다. 이 경우 컨벌루션 베이스 모델은 각 이미지와 관련된 모든 특징을 추출하며 주어진 추출된 특징을 가지고 이미지 클래스를 결정하는 분류기를 훈련합니다.
- **사전 훈련된 모델을 미세 조정하기**: 성능을 더욱 향상시키기 위해 사전 훈련된 모델의 최상위 계층을 미세 조정을 통해 새 데이터셋으로써 재사용 할 수 있습니다. 이 경우 모델이 주어진 데이터셋에 맞는 상위 레벨의 특징을 학습 할 수 있도록 가중치를 조정합니다. 이 기술은 일반적으로 훈련 데이터셋의 규모가 크고, 사전 훈련된 모델이 사용했던 원래 데이터셋과 매우 유사한 경우에 권장됩니다.



