# Introduction

> **Word Based Encoding**
>
> 기계학습 모델은 벡터(숫자 배열)를 입력으로 사용한다. 텍스트로 작업할 때 가장 먼저 해야할 일은 문자열을 모델에 공급하기 전에, 문자열을 숫자로 변환(텍스트 벡터화)하는 것이다.
>
> Intro에서는 각 단어를 고유의 한 번호로 인코딩 하는 방법을 소개했다. 예를 들어 [cat, mat, on, sat, the]라는 단어 리스트가 있을 때, 'cat'에 1을 할당하고, ' mat'에 2를 할당한다. 나머지도 이와 같은 방식으로 숫자를 할당한다. 그러면, *"The cat sat on the mat"* 라는 문장을 [5,1,4,3,5,2]와 같은 고밀도 벡터로 인코딩할 수 있다.
>
> **단점**
>
> * 정수 인코딩은 임의적이다. (단어 간의 관계 포착하지 않음)
> * 정수 인코딩은 모델이 해석하기 어려울 수 있다. 예를 들어 선형 분류기는 각 특성에 대한 단일 가중치를 학습한다. 두 단어의 유사성과 해당 인코딩의 유사성 간에는 관계가 없기 때문에, 이 가중치 조합은 의미가 없다.
>
> ~~~python
> from tensorflow.keras.preprocessing.text import Tokenizer
> 
> sentences = [
>     'i love my dog',
>     'I, love my cat', # 대문자 ---> 소문자로 표시
>     'You love my dog!'# 느낌표 ---> 삭제
> ]
> 
> tokenizer = Tokenizer(num_words = 100) # 100개의 일반적인 단어 사용
> tokenizer.fit_on_texts(sentences)
> word_index = tokenizer.word_index
> print(word_index)
> ~~~
>
> ~~~python
> import tensorflow as tf
> from tensorflow import keras
> 
> 
> from tensorflow.keras.preprocessing.text import Tokenizer
> from tensorflow.keras.preprocessing.sequence import pad_sequences
> 
> sentences = [
>     'I love my dog',
>     'I love my cat',
>     'You love my dog!',
>     'Do you think my dog is amazing?'
> ]
> 
> tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>") 
> tokenizer.fit_on_texts(sentences)
> word_index = tokenizer.word_index
> 
> sequences = tokenizer.texts_to_sequences(sentences)
> 
> padded = pad_sequences(sequences, maxlen=5)
> print("\nWord Index = " , word_index)
> print("\nSequences = " , sequences)
> print("\nPadded Sequences:")
> print(padded)
> 
> 
> # Try with words that the tokenizer wasn't fit to
> test_data = [
>     'i really love my dog',
>     'my dog loves my manatee'
> ]
> 
> test_seq = tokenizer.texts_to_sequences(test_data)
> print("\nTest Sequence = ", test_seq)
> 
> padded = pad_sequences(test_seq, maxlen=10)
> print("\nPadded Test Sequence: ")
> print(padded)
> ~~~
>
> **Tokenize**
>
> * 문장을 단어 또는 형태소 단위로 토큰화.
> * 문장이란 것은 단어의 연속이기 때문에 리스트로 표현할 수 있음
> * 토큰은 문장, 단어, character, 형태소가 될 수 있음
>
> **OOV**
>
> 단어셋에 없는 단어를 만나면 아예 Indexing 자체를 할 수 없게 된다. 이러한 문제를 Out of Vocabulary(OOV)라고 부른다. oov_token을 이용하여 외부 단어들을 모두 oov로 인덱싱해준다.

